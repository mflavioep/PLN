{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para ler um arquivo texto e atribuir o texto em uma variável\n",
    "\n",
    "def ler(arq_texto):\n",
    "    arquivo = open(arq_texto, 'r', encoding='utf-8')\n",
    "    texto = arquivo.read()\n",
    "    arquivo.close()\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219702\n"
     ]
    }
   ],
   "source": [
    "texto = ler('Corpora\\\\Ubirajara.txt') #Atenção a barra dupla para abertura do arquivo\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordanciador\n",
    "Os concordanciadores são programas que buscam por expressões em um corpus e criam listagens dos resultados, incluindo um número determinado de outras expressões antes e depois daquela que é buscada, a fim de situá-la em um contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCORDANCIADOR simples\n",
    "\n",
    "def concordanciador(alvo, texto):\n",
    "    texto = texto.replace('\\n', ' ')\n",
    "    texto = texto.replace('\\t', ' ')\n",
    "\n",
    "    ocorrencias = list()\n",
    "    resultado = texto.find(alvo, 0)\n",
    "    while resultado > 0:\n",
    "        pos_inicial = resultado - (40 -len(alvo) // 2)\n",
    "        ocorrencias.append(texto[pos_inicial : pos_inicial + 80])\n",
    "        resultado = texto.find(alvo, resultado + 1)\n",
    "\n",
    "    return ocorrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e, Jaguarê não te quer matar como a serpente que ataca o descuidado caçador. Dez\n",
      "reiro invencivel que tem por arma a serpente. Reconhece o teu vencedor, Pojucan,\n",
      "zes ella escapou-lhe da mão, como a serpente das garras do gavião.  «Mais uma ve\n",
      "reiro terrivel que tem por arma uma serpente.»         *       *       *       *\n",
      "«Agora eu queria ter no coração uma serpente para morder aquella que me roubou o\n",
      " porque elle tem os olhos da grande serpente de fogo, que vôa como o raio de Tup\n",
      "uerreiros ferozes, filhos da grande serpente do mar.  Um dia esses guerreiros sa\n",
      "la, muito cheiroza, a qual a grande serpente creava no bucho.  Os Tupinambás faz\n",
      " na tromba. Elle fujia, esticando a serpente; e a serpente encolhendo-se o arras\n",
      "le fujia, esticando a serpente; e a serpente encolhendo-se o arrastava até á bei\n",
      " pelo meio.  O velho tapir rompeu a serpente como se rompe uma corda de piassaba\n",
      " a liberdade. Ella tem a astucia da serpente e seu veneno.  --Eu era a cobra d'a\n",
      "to.  Vem depois Arariboia, a grande serpente das lagôas; Cauatá, o corredor das \n",
      "uas pontas girou em sua mão, como a serpente que se enrosca nos ares silvando.  \n",
      "iro invencivel que tem por arma uma serpente.  «Eu sou Ubirajara, o senhor das n\n"
     ]
    }
   ],
   "source": [
    "resultados = concordanciador('serpente', texto)\n",
    "for i in resultados:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = texto.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problemas de pontuação e caracteres maiúsculos\n",
    "\n",
    "def limpar(lista):\n",
    "    lixo = '.,:;?!\"`()[]{}\\/|#$%^&*'\n",
    "    return[X.strip(lixo).lower() for X in lista]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'maça', 'abaca.te', 'melancia']\n"
     ]
    }
   ],
   "source": [
    "corpus_sujo = ['banana', 'maça.', 'abaca.te', ':MeLancia' ]\n",
    "print(limpar(corpus_sujo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problemas de pontuação e caracteres maiúsculos\n",
    "# Inclusão do isalpha\n",
    "\n",
    "def limpar(lista):\n",
    "    lixo = '.,:;?!\"`()[]{}\\/|#$%^&*'\n",
    "    passo1 = [X.strip(lixo).lower() for X in lista]    \n",
    "    return[X for X in passo1 if X.isalpha() or '-' in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['banana', 'maça', 'melancia', 'busca-se']\n"
     ]
    }
   ],
   "source": [
    "corpus_sujo = ['banana', 'maça.', 'abaca.te', ':MeLancia', 'busca-se' ]\n",
    "print(limpar(corpus_sujo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpando o corpus\n",
    "len(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palavras = limpar(palavras)\n",
    "len(palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulário e riqueza lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6944"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Análises quantitativas\n",
    "# Estabelecimento do vocabulário do corpus, as palavras não podem ser repetidas\n",
    "\n",
    "vocabulario = set(palavras)\n",
    "len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1928835310130274\n"
     ]
    }
   ],
   "source": [
    "# Cálculo da riqueza lexical\n",
    "\n",
    "riqueza = len(vocabulario) / len(palavras)\n",
    "print(riqueza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação de palavras por ocorrência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocorrencias(lista_palavras):\n",
    "    dicionario = defaultdict(int)\n",
    "    for p in lista_palavras:\n",
    "        dicionario[p] += 1\n",
    "    return dicionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \t 1473\n",
      "o \t 1378\n",
      "de \t 1199\n",
      "que \t 1118\n",
      "e \t 918\n",
      "do \t 685\n",
      "da \t 624\n",
      "os \t 490\n",
      "para \t 346\n",
      "não \t 335\n"
     ]
    }
   ],
   "source": [
    "dic = ocorrencias(palavras)\n",
    "mf = sorted(dic.items(), key=lambda tupla:tupla[1], reverse=True) [:10] # [:10] lista as 10 ocorrencias\n",
    "for palavra, n in mf:\n",
    "    print(palavra,'\\t',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que as palavras de maior ocorrência são as palavras funcionais, isto é, \n",
    "# palavras de reduzida contribuição semântica ou nocional, que servem apenas para\n",
    "# estabelecer relações de outras palavras entre si.\n",
    "# \n",
    "# Pela fraca contribuição semântica é comum que estas palavras sejam eliminadas \n",
    "# do CORPUS nas análises computacionais relacionadas ao significado.\n",
    "# São tidas como \"palavra vazia\" (stop words), que também podem incluir substantivos e verbos banais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\mflav\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\mflav\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\mflav\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mflav\\anaconda3\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mflav\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mflav\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "vazias = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O resultado deve ser 0, ou seja, todas as dez palavras mais frequentes do livro são vazias\n",
    "frequentes_plenas = [X for X in mf if X[0].lower() not in vazias]\n",
    "len(frequentes_plenas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \t 1473\n",
      "o \t 1378\n",
      "de \t 1199\n",
      "que \t 1118\n",
      "e \t 918\n",
      "do \t 685\n",
      "da \t 624\n",
      "os \t 490\n",
      "para \t 346\n",
      "não \t 335\n",
      "dos \t 327\n",
      "se \t 290\n",
      "as \t 265\n",
      "como \t 243\n",
      "guerreiro \t 235\n",
      "um \t 229\n",
      "seu \t 215\n",
      "em \t 212\n",
      "na \t 205\n",
      "mais \t 205\n"
     ]
    }
   ],
   "source": [
    "mf = sorted(dic.items(), key=lambda tupla:tupla[1], reverse=True) [:20] # [:20] lista as 10 ocorrencias\n",
    "for palavra, n in mf:\n",
    "    print(palavra,'\\t',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O resultado deve ser 1, a palavra guerreiro não é uma palavra vazia\n",
    "frequentes_plenas = [X for X in mf if X[0].lower() not in vazias]\n",
    "len(frequentes_plenas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hápax legômena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hápax legômena, no plural, ou Hápax legômenon, no singular, é o termo técnico usado\n",
    "# para designar as palavras com única ocorrência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4365"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hapax = [X for X in palavras if palavras.count(X) == 1]\n",
    "len(hapax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Um dos problemas à identificar são as formas raras de conjugação verbal, assim reduz-se\n",
    "# todas as palavras do corpus as sua raízes morfológicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\mflav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O stemmer do NLTK reduz as palavras as suas raízes\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "raizes = [stemmer.stem(X) for X in set(palavras)]\n",
    "hapax = [X for X in raizes if raizes.count(X) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6944 \t 3210\n"
     ]
    }
   ],
   "source": [
    "print(len(raizes),'\\t',len(hapax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val', 'nenhum', 'espóz', 'extingu', 'conquistal-', 'saí', 'acontec', 'mov', 'colon', 'venh', 'damag', 'retin', 'comvosc', '--p', 'relaç', 'corrent', 'bast', 'campeã', 'vaz', '_hospede_.--', 'tamb', 'gumilh', 'disting', 'ident', 'deix', 'cativ', 'cord', 'recei', 'eu', 'appearing'] \n",
      " ['espóz', 'extingu', 'retin', 'comvosc', '--p', 'corrent', '_hospede_.--', 'tamb', 'gumilh', 'disting', 'ident', 'eu', 'appearing', 'prova-', 'caiu', 'particip', 'pôr', 'bertrand', 'recostou-s', 'secur', 'invaria', 'adversari', 'côté', 'encor', 'ocean', '«mata-m', 'erguia-s', 'abrir', '--torn', 'ensuring']\n"
     ]
    }
   ],
   "source": [
    "print(raizes[:30],'\\n',hapax[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6247119815668203\n"
     ]
    }
   ],
   "source": [
    "# Uma outra medida de riqueza lexical do corpus seria dada pela divisão do conjunto\n",
    "# de raízes distintas pela contagem absoluta de ocorrências de raízes, ou seja, \n",
    "# divisão de types por tokens\n",
    "\n",
    "print(len(set(raizes)) / len(raizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatística Descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3768297091b44acbe473c5b73c7cead172651c8d7375651eef94c19d1bc8b42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
